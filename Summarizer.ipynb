{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install the requirements**"
      ],
      "metadata": {
        "id": "asbvaMRU4TOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "7NJw2X8F4Scl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import time"
      ],
      "metadata": {
        "id": "yjALUmgXZWWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load your file**"
      ],
      "metadata": {
        "id": "2DhNvcj94Xom"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XOYHYLm8aa4"
      },
      "outputs": [],
      "source": [
        "filepath=\"/content/document.txt\"\n",
        "openai.api_key  = 'Your_OpenAI_Key'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DY2WJmuySyH"
      },
      "outputs": [],
      "source": [
        "with open(filepath, 'r') as out:\n",
        "  txt = out.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ChatGPT helper function**"
      ],
      "metadata": {
        "id": "T1oo2Ezx4c5x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CfKbYD7y8aA"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REWP29r6y3Uk"
      },
      "source": [
        "**Cleaning text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW6pdQyw1Y3f"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtkQ4dHKUdMu"
      },
      "outputs": [],
      "source": [
        "txt = ' '.join(txt.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7KtYhVGbGXh"
      },
      "outputs": [],
      "source": [
        "length = len(txt.split())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "length"
      ],
      "metadata": {
        "id": "IDmr9NH4eEZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba45902-9bbe-4ce1-9f07-4367ab7f67bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6096"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt"
      ],
      "metadata": {
        "id": "4ydXUfTFEE5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = txt.split()"
      ],
      "metadata": {
        "id": "t4J9gFEnxDF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4zafhq3W44_"
      },
      "source": [
        "**Making sections from the text**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "promptt = '''\n",
        "'''"
      ],
      "metadata": {
        "id": "qfZaEIdA65hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "summary = ''\n",
        "text = text.split()\n",
        "#Break the text into 2000 words\n",
        "idx = 0\n",
        "while(idx<len(txt)):\n",
        "  idx_start = idx\n",
        "  idx_end = idx_start + 2000\n",
        "\n",
        "  #Search the full stop so that we have a full paragraph that makes sense\n",
        "  for x in range(idx_end, len(txt)):\n",
        "    if(txt[x][-1] == '.'):\n",
        "      idx_end+=1\n",
        "      break\n",
        "    else:\n",
        "      idx_end += 1\n",
        "\n",
        "  #Convert the list into a string\n",
        "  text = ' '.join(txt[idx_start:idx_end])\n",
        "  idx = idx_end\n",
        "\n",
        "  #Hit the API to get response\n",
        "  prompt = f\"\"\" {promptt}\n",
        "\n",
        "  Text: ```{text}```\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    response = get_completion(prompt)\n",
        "  except:\n",
        "    response = get_completion(prompt)\n",
        "  #Store the result in the summary variable for each section\n",
        "  summary = summary + ' ' + response +'\\n\\n'\n",
        "  result.append(response)\n",
        "  time.sleep(19)  #You can query the model only 3 times in a minute for free, so we need to put some delay\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBnwxNYZdu5q",
        "outputId": "b84b411b-d73e-4ce2-8b08-f273edb09ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2008\n",
            "4015\n",
            "6023\n",
            "8023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save the summary of different sections**"
      ],
      "metadata": {
        "id": "HPwWbWHPcZXK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8t-o880y8iF"
      },
      "outputs": [],
      "source": [
        "with open('summary2.txt', 'w') as out:\n",
        "    out.write(summary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d0z6_b48kM_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hit the API again to get the final summary**"
      ],
      "metadata": {
        "id": "6K_KwYOdcejE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQhrdk19IvEC"
      },
      "outputs": [],
      "source": [
        "prompt = f'''The text entered is the summary from a text. I want you to......\n",
        "Text: ```{summary}```\n",
        "'''\n",
        "\n",
        "try:\n",
        "  response = get_completion(prompt)\n",
        "except:\n",
        "  response = get_completion(prompt)\n",
        "article = response +'\\n\\n'\n",
        "result.append(response)\n",
        "time.sleep(19)  #You can query the model only 3 times in a minute for free, so we need to put some delay\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pc4dXFRPiOvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save the final summary**"
      ],
      "metadata": {
        "id": "fA41LkHBco7N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJh3TnJLIvjj"
      },
      "outputs": [],
      "source": [
        "with open('Final_summary.txt', 'w') as out:\n",
        "    out.write(Final_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNJg5nMzdtjE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "a176ac03-723b-4a65-ebe2-83685ef0ea86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Introduction:\\n\\nDatahour is an online 1-hour webinar started by Analytics Vidhya, where industry experts share their knowledge and experience in the field of data science and artificial intelligence. In one such session, Ravi Theja, an accomplished Data Scientist at Glance-Inmobi, shared his expertise in building and deploying cutting-edge machine learning models for recommender systems, NLP applications, and Generative AI. With a Master\\'s degree in Computer Science from IIIT-Bangalore, Ravi has solidified his foundation in the field of data science and artificial intelligence.\\n\\nThe session focused on the Llama Index and how it can be used to build US systems with private data and evaluate QA systems. In this blog post, we will discuss the key takeaways from the session and provide a detailed explanation of the Llama Index and its applications.\\n\\nTable of Contents:\\n\\n1. What is the Llama Index?\\n2. Challenges of dealing with large amounts of text data\\n3. Llama Index Structure\\n4. Generating a response given a query on indexes\\n5. Querying and summarizing documents using a specific response mode\\n6. Indexing CSV files and how they can be retrieved for queries\\n7. Use of different textures and data sources in creating indexes and query engines\\n8. Evaluation framework for a question and answer system\\n9. Conclusion\\n10. Frequently Asked Questions\\n\\n1. What is the Llama Index?\\n\\nThe Llama Index is a solution that acts as an interface between external data sources and a query engine. It has three components: a data engine, indexing or data success, and a query interface. The data connectors provided by Llama index allow for easy ingestion of data from various sources, including PDFs, audio files, and CRM systems. The index stores and indexes the data for different use cases, and the query interface pulls up the required information to answer a question. The Llama index is useful for various applications, including sales, marketing, recruitment, legal, and finance.\\n\\n2. Challenges of dealing with large amounts of text data\\n\\nThe text discusses the challenges of dealing with large amounts of text data and how to extract the right information to answer a given question. Private data is available from various sources, and one way to use it is to fine-tune LLMs by training on your own data. However, this requires a lot of data preparation effort and lacks transparency. Another way is to use prompts with a context to answer questions, but there is a token limitation.\\n\\n3. Llama Index Structure\\n\\nThe Llama index structure involves creating an overview of data through indexing documents. The indexing process involves chunking the text document into different nodes, with each node having an embedding. A retriever helps to retrieve documents for a given query, and a query engine manages retrieval and census for the query. The Llama index has different types of indexes, with the vector store index being the simplest. The document is divided into nodes, and an embedding is created and stored for each node. When querying, a query embedding is created, and the top nodes similar to the query are retrieved and used to generate a response through the sales model. Llama is free and integrates with the collapse.\\n\\n4. Generating a response given a query on indexes\\n\\nThe text discusses the process of generating a response given a query on indexes. The author explains that the default value of the test store indexing is set to one, meaning that if a vector is used for indexing, it will only take the first node to generate an answer. However, if the list index is used, the LM will iterate over all nodes to generate a response. The author also explains the create and refine framework used to generate responses, where the LM regenerates the answer based on the previous answer, query, and node information. The text mentions that this process is useful for semantic search and can be achieved with just a few lines of code.\\n\\n5. Querying and summarizing documents using a specific response mode\\n\\nThe text discusses how to query and summarize documents using a specific response mode called \"3 summarize\" provided by the A Mindex tool. The process involves importing necessary libraries, loading data from various sources such as web pages, PDFs, and Google Drive, and creating a vector store index from the documents. The text also mentions a simple UI system that can be created using the tool. The response mode allows for querying on documents and providing summaries of the article. The text also mentions the use of source notes and similarity support for answering questions.\\n\\n6. Indexing CSV files and how they can be retrieved for queries\\n\\nThe text discusses indexing CSV files and how they can be retrieved for queries. If a CSV file is indexed, it can be retrieved for a query, but if it is indexed with one row having one data point with different columns, some information may be lost. For CSV files, it is recommended to ingest the data into a WSL database and use a wrapper on top of any SQL database to perform text U SQL. One document can be divided into multiple chunks, and each chunk is represented as one node along with the embedding and text. The text is split based on different types of texts, such as cars, computers, and sentences.\\n\\n7. Use of different textures and data sources in creating indexes and query engines\\n\\nThe text discusses the use of different textures and data sources in creating indexes and query engines. When data sources are in different stories, indexes can be created from each source and then combined into a composite graph. This allows for relevant nodes to be pulled from both indexes when querying. Additionally, the query engine can be used to split a given query into multiple questions to generate a meaningful answer. The notebook provides an example of how to use these techniques.\\n\\n8. Evaluation framework for a question and answer system\\n\\nThe Lambindex system has both service context and storage context. Service context helps define different LM models or embedding models, while storage context stores notes and chunking documents. The system reads and indexes documents, creates an object for query transformation, and uses a multi-step query engine to answer questions about the author. For complex questions, the system splits them into multiple queries and generates a final answer based on the answers from the intermediate queries. However, evaluating the system\\'s answers is critical, especially when dealing with large enterprise-level data sources. It is not feasible to create questions and answers for each document, so evaluation becomes crucial.\\n\\nThe evaluation framework discussed in the text aims to simplify the process of generating questions and evaluating answers. The framework has two components: a question generator and a response evaluator. The question generator creates questions from a given document, and the response evaluator checks whether the answers generated by the system are correct or not. The response evaluator also checks whether the source node information matches the response text and the query. If all three are in line, the answer is considered correct. The framework aims to reduce the time and cost associated with manual labeling and evaluation.\\n\\n9. Conclusion\\n\\nIn conclusion, the Llama Index is a powerful tool that can be used to build US systems with private data and evaluate QA systems. It provides an interface between external data sources and a query engine, making it easy to ingest data from various sources and retrieve the required information to answer a question. The Llama index is useful for various applications, including sales, marketing, recruitment, legal, and finance. The evaluation framework discussed in the text simplifies the process of generating questions and evaluating answers, reducing the time and cost associated with manual labeling and evaluation.\\n\\n10. Frequently Asked Questions\\n\\nQ1. What is the Llama Index?\\nA1. The Llama Index is a solution that acts as an interface between external data sources and a query engine. It has three components: a data engine, indexing or data success, and a query interface.\\n\\nQ2. What are the applications of the Llama Index?\\nA2. The Llama index is useful for various applications, including sales, marketing, recruitment, legal, and finance.\\n\\nQ3. How can the Llama Index be used to generate responses given a query on indexes?\\nA3. The Llama Index can be used to generate responses given a query on indexes by using the create and refine framework, where the LM regenerates the answer based on the previous answer, query, and node information.\\n\\nQ4. How can CSV files be indexed and retrieved for queries?\\nA4. CSV files can be indexed and retrieved for queries by ingesting the data into a WSL database and using a wrapper on top of any SQL database to perform text U SQL.\\n\\nQ5. What is the evaluation framework for a question and answer system?\\nA5. The evaluation framework for a question and answer system aims to simplify the process of generating questions and evaluating answers. The framework has two components: a question generator and a response evaluator.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "Final_summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IZLUJ2dzO1ka"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}